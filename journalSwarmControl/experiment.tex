
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Particle Swarm Object manipulation}\label{sec:exp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section analyzes an \emph{object manipulation} task attempted by our hybrid, hysteresis-based controllers. The swarm must deliver the object to the goal region. 
%We assume that particles pushing the object move slower than unencumbered particles, and that pushing with more particles increases the object speed
We assume Coulomb and viscous friction parameters such that the object can be moved by particle motion.  Increasing the number of pushing particles increases the object speed.
%Although one particle might be able to move the object, it requires a long time for it to complete the task and probability of getting stuck when there is not enough particles is high.
 To solve this object manipulation task we divide the task into three components: 1) designing a policy for the object, 2) pushing the object with a compliant swarm, and 3) managing outliers.

The table below summarizes the simulation results for each 10 successful trials:
\begin{center}
\begin{tabular}{ c c }
\hline
Method & Result, mean$\pm$std (s)\\ [0.5ex] 
\hline \hline
Value Iteration (VI) & 367 $\pm$ 253 \\ 
\hline
VI + Potential Fields (PF) & 271 $\pm$ 267 \\  
\hline
VI + Outlier Rejection (OR) & 245 $\pm$ 135\\
\hline
BFS + PF + OR & 183 $\pm$ 179 \\
\hline
VI + PF + OR & 90 $\pm$ 35 \\[0.5ex]
\hline
\end{tabular}
\end{center}
\subsection{Learning a policy for the object}\label{subsec:objectpolicy}

To design the policy we first discretize the environment. 
In \cite{ShahrokhiIROS2015}, we used breadth-first search (BFS) on this discretized grid, but using workspace BFS fails to account for the hull of the object and will suggest moves that can cause collisions with the workspace. A configuration-space BFS approach avoids that problem but still fails to model uncertain actuation of the object by the swarm.

To solve both these problems, this paper models object movement as a Markov Decision Process (MDP) with non-deterministic movement.  
  Value iteration,  as described by Thrun \cite{Thrun2005}, is used to learn an \emph{optimal policy}.
   At each state the object can be commanded to move in one of eight directions with a small probability of moving in a wrong direction. 
 
% A corresponding reward function gives a high reward to the goal state,  
% a large negative reward to states including obstacles, and a small negative reward to all other states.
The reward function $r(x,\mathbf{u})$ is defined as
\begin{align}
r(x,\mathbf{u}) &=  \left\{
\begin{array}{ll}
     +100, &  \textrm{if } \mathbf{u} \textrm{ leads to goal state}\\
      -100, & \textrm{if } \mathbf{u} \textrm{ leads to an obstacle state} \\
      -1, & \textrm{otherwise}\\
\end{array} 
\right.
\end{align}
 where $x$ is the current state and $\mathbf{u}$ is the action.   %cumulative expected
  Value iteration computes $\hat{V}(x)$, the expected discounted sum reward if the optimal policy is implemented, for the object starting in each state $x$. The optimal policy is %the control action that, in probability, results in the largest value:
   \begin{align} \mathbf{D}(x) = \arg\max_{\mathbf{u}}   [ r(x,\mathbf{u}) + \sum\limits_{i=1}^N \hat{V}(x_i) p(x_i|x, \mathbf{u})].  \label{eq:OptimalPolicy}
   \end{align}
   The value function $\hat{V}(x_j) $ is calculated by computing the value $\hat{V}$ for all $N$ states and iterating until convergence:
\begin{align}
\text{for }&\text{$j=1$ to $N$ do} \nonumber \\
&\hat{V} (x_j) = \gamma \max_{\mathbf{u}} [r(x_j,\mathbf{u}) + \sum\limits_{i=1}^N \hat{V}(x_i) p(x_i| x_j,\mathbf{u})] \label{eq:ValueIteration} \nonumber\\
\text{end}& 
\end{align}
In our experiments $\gamma = 0.97$, and \eqref{eq:ValueIteration} was iterated 200 times. A {\sc Matlab} implementation of this algorithm is available at \cite{Becker2015MDP}.
%Rather than a shortest distance to goal, $\nabla \mathbf{M}_{MDP}$  gives the approximate minimum cost to goal.

$\mathbf{M}_{\rm{BFS}}$ and the value function are shown in Fig.~\ref{fig:BFSGradient}. 
In 10 simulations with 100 particles, pushing the object to goal using BFS required 183$\pm$179 s while Policy Iteration required 90$\pm$35 s (mean$\pm$std).
%\todo{(report results of 10 simulations with BFS and with PI.  Give mean and std)}

\begin{figure}
\centering
\renewcommand{\figwid}{3cm}
\begin{overpic}[height=\figwid]{GradientView.png}
\end{overpic}
\begin{overpic}[height=\figwid]{PolicyIter.png}
\end{overpic}
\begin{overpic}[height=\figwid]{MDPmap.pdf}
\end{overpic}
\vspace{-0.5em}
\caption{\label{fig:BFSGradient} BFS finds the shortest path for the moveable object to compute gradient vectors (left). Modeling as an MDP enables encoding penalties for being near obstacles. (Middle) The control policy from value iteration. (Right) The vision algorithm detects obstacles in the hardware setup. This map is used to produce the value function and control policy shown. 
%\vspace{-2em}
}
\end{figure}


\subsection{Potential fields for swarm management with a compliant manipulator}

When the swarm is in front of the object, control law \eqref{eq:PDcontrolPosition} pushes the object backwards.  To fix this, we implement a potential field approach inspired by Spong \cite{spong2008robot} that attracts the swarm to the intermediate goal, but repulses the swarm from in front of the object, as shown in Fig.~\ref{fig:potentialField}.
The repulsive potential field is centered at $\mathbf{b}$, the object's COM, and is active in a circular sector of angular width 2$\theta$ and radius $d_0$ aligned with $\mathbf{D}(\mathbf{b})$. $\mathbf{D}(\mathbf{b})$ is the desired direction of motion from~\eqref{eq:OptimalPolicy}.
\begin{align}
\mathbf{d} &=  [\bar{x},\bar{y}] - \mathbf{b}, \qquad   \phi =\cos^{-1}\left( \frac{ \mathbf{D}(\mathbf{b}) \cdot  \mathbf{d}  }{\norm{\mathbf{D}(\mathbf{b})} \cdot \norm{\mathbf{d}} } \right), \nonumber \\ 
F_{\rm{att}} &= -\zeta \frac{\mathbf{d}  }{ \norm{\mathbf{d}}}, \nonumber\\
 F_{\rm{rep}} &=  \left\{
\begin{array}{ll}
      \eta( \frac{1}{\norm{\mathbf{d}}}- \frac{1}{d_0}) \frac{1}{\norm{\mathbf{d}}^2} \mathbf{d}, ~& \norm{\mathbf{d}} \leq d_0~\wedge~\phi <  \theta \\
      0, & \textrm{ otherwise} \nonumber \\
\end{array},
\right.\\
F_{\rm{pot}} &= F_{\rm{att}} + F_{\rm{rep}} \label{eq:potentialfield}, \nonumber\\
[x_{\rm{goal}}&, y_{\rm{goal}}] =  [\bar{x},\bar{y}]  + \frac{F_{\rm{pot}}}{\norm{F_{\rm{pot}}}}.
\end{align}
Here  $\eta$, $\zeta$, and $d_0$ are all positive parameters that scale the forces and $\norm{\mathbf{d}}$ is the distance from the swarm mean $ [\bar{x},\bar{y}]$ to the object COM.
In simulations, $\theta =  \pi/2$,  $\eta  = 75$, $\zeta = 2$ and $d_0 = 3$. Because the kilobots have a slower time constant, they use $\theta =  \pi/2$,  $\eta  = 50$, $\zeta = 1$ and $d_0 = 7.5$. 

In 10 simulations with 100 particles, pushing the object to goal without a repulsive potential field failed in two of twelve runs. No failures occurred with the repulsive potential field.  Of successful trials, completion time without repulsive potential fields required 245$\pm$135 s while using repulsive potential fields required 90$\pm$35 s (mean$\pm$std).
%In 10 simulations with 100 robots, completion time without repulsive potential fields required $\mu=245, \sigma=135$s while using repulsive potential fields required $\mu=90, \sigma=35$.

%\todo{report results of 10 simulations with and without Potential fields.  Give mean and std}

\begin{figure}
\centering
\begin{overpic}[width=1\columnwidth]{PotentialField.pdf}\end{overpic}
%\todo{I like the 'target' symbol, but it is not self-documenting.  We need a legend explaining the min and max variance ellipses, the goal region, the variance, the mean, the object COM, and the target mean position.  I think these are easiest to make in powerpoint.
%Please use the same color and line style for the variance min and max as you use in Figure 4.
%}
%{blockpushingImageWithMeanAndVarianceOverlay.png}
\caption{\label{fig:potentialField} (Left) The attractive field is centered behind the object's COM. (Middle) The repulsive field is centered at the object's COM. (Right) Combining these forces prevents the swarm from pushing the object backwards.}
\end{figure}

\subsection{Outlier rejection}\label{subsec:OutlierRejection}

The variance controller in Alg.~\ref{alg:MeanVarianceControl} is a greedy algorithm that is susceptible to outliers. 
Our controller in \cite{ShahrokhiIROS2015} failed in $14\%$ trials, some particles were unable to reach the object because workspace obstacles were blocking them. This failure rate increases if  object weight increases or ground-robot friction increases. The mean and covariance calculations \eqref{eq:meanVar} included all particles in the workspace. Particles that cannot reach the object due to obstacles skew these calculations. The state machine in Fig. \ref{fig:Region}a solves this problem by creating two states for the maze: either main or transfer. Each state has a set of regions representing a discretized visibility polygon. Whenever the object crosses a region boundary the state toggles. The \emph{main} regions are generated by extending obstacles until they meet another obstacle. The \emph{transfer} regions are perpendicular to obstacle boundaries, and act as a buffer between two main regions.

Fig.\ \ref{fig:Region}b shows the regions for the main state. The object is in region 1. An indicator function is applied to \eqref{eq:meanVar} so only robots inside region 1 are counted.  This filtering increases experimental success because the mean calculation only includes nearby robots that can directly interact with the object. 
When the object leaves main region 1 the state switches to transfer. The transfer regions are shown in Fig.\ \ref{fig:Region}c.  The object is in transfer region 1, so only robots in transfer region 1 are included in the mean and covariance calculations. 
 The robots should push the object to the left. Without filtering using regions, the red circle is the mean and the algorithm would instruct the robots to push the object up. The black circle shows the filtered mean and the algorithm instructs the robots to push the object directly left.

In 10 simulations with 100 robots, completion time without outlier rejection required 271$\pm$267 s while using outlier rejection required 90$\pm$35 s (mean$\pm$std).
%\todo{report results of 10 simulations with and without regions.  Give mean and std}


\begin{figure}
\begin{center}
	\begin{overpic}[width=0.9\columnwidth]{StateMachine.pdf}\put(-4,25){\textbf{a}}\end{overpic}\\
	~~\begin{overpic}[width=0.45\columnwidth]{MainRegions.pdf} %,grid=true
	\put(-6,85){\textbf{b}}
	\end{overpic}
	~~
	\begin{overpic}[width=0.45\columnwidth]{TransferRegions.pdf}
	\put(-6,85){\textbf{c}}
	\end{overpic}
\end{center}
\vspace{-0.5em}
\caption{\label{fig:Region}  Outlier rejection state machine and regions.
}
\end{figure}

\subsection{Simulation results}\label{sec:AlgObjectManipulation}
 We use the hybrid hysteresis-based controller in Alg.~\ref{alg:MeanVarianceControl}  to track the desired position, while maintaining sufficient robot density to move the object by switching to minimize variance whenever variance exceeds a set limit:  $0.003 W$ and $0.006 W$ were added to the min and max variance limits from \eqref{eq:sigMaxMin}.
 The minimize variance control law \eqref{eq:PDcontrolVariance} is slightly modified to choose the nearest corner further from the goal than the object with an obstacle-free straight-line path to the object. 
The control algorithm  for object manipulation is listed in Alg.~\ref{alg:BlockPushing}. 

In rare cases during simulations the swarm may become trapped in a local minimum of \eqref{eq:potentialfield}.
If the swarm mean position does not change for five seconds, the swarm is assumed to be in a local minimum and is commanded to move toward the previous corner. As soon as the mean position changes, normal control resumes.

\begin{figure}
%  \vspace{-20pt}
  \begin{center}
\begin{overpic}[width=0.4\columnwidth]{AllShapes.pdf}\end{overpic}
  \end{center}
%  \vspace{-1em}
\caption{\label{fig:Shapes} The six equal-area objects tested in simulation. %Six object shapes were tested.
\vspace{-1em}
}
\end{figure}



%\todo{add the timeout function: I added two lines.}

\begin{algorithm}
\caption{Object-manipulation controller for a robotic swarm.}\label{alg:BlockPushing}
\begin{algorithmic}[1]
\Require Knowledge of moveable object's center of mass $\mathbf{b}$; swarm mean $[\bar{x},\bar{y}]$ and variance $[\sigma_x^2, \sigma_y^2]$, each calculated using the regions function from \S \ref{subsec:OutlierRejection};  map of the environment
\State Compute optimal policy for object, according to \S \ref{subsec:objectpolicy}
\While{is not in goal region}
\State $\sigma^2 \gets \max{(\sigma_x,\sigma_y)}$
\If {$\sigma^2 > \sigma_{\rm{max}}^2$}
\While{$\sigma^2 > \sigma_{\rm{min}}^2$}
\State $ [x_{\rm{goal}}, y_{\rm{goal}}] \gets $ nearest corner in region
\State Apply \eqref{eq:PDcontrolPosition} to move toward $[x_{\rm{goal}}, y_{\rm{goal}}]$
\EndWhile
\Else  
%\If {$\mathrm{distance}( \mathbf{b}, [x_{goal}, y_{goal}] ) > k_1 r_b$}
%	\State$r_p \gets k_2 r_b$  \Comment{guarded move}
%	\Else
%	\State$r_p \gets k_3 r_b$  \Comment{pushing move}
%	\EndIf
%\State$r_p \gets k_3 r_b$  \Comment{pushing move}
\State Calculate $\mathbf{D}(\mathbf{b})$  \Comment{direction for object at $\mathbf{b}$}
\State Apply \eqref{eq:potentialfield}   \Comment{potential field for swarm}
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}


%\subsection{Automated object manipulation (simulation)}
Fig.~\ref{fig:story} shows snapshots during an execution of this algorithm in simulation.
 We also have tested two other mazes with ``E'' shape and a ``Spiral'' without changing the algorithm to illustrate the capability of the algorithm in Fig.~\ref{fig:Emaze}. Fig.~\ref{fig:DifferentMazes} shows the results of all the three different mazes. E-shape maze has the least average time because the path to the goal is shorter. 
 Experimental results of parameters sweeps are summarized in Fig.~\ref{fig:AutoVeryParam}.  Each trial measured the time to deliver the object to the goal location.  The default parameter settings used 100 robots, a normalized weight of 1, a hexagon shape, and Brownian noise (applied once each simulation step) with $W=5$.  

\begin{figure}
\centering
%\renewcommand{\figwid}{0.19\columnwidth}
%\href{http://youtu.be/tCej-9e6-4o}{\begin{overpic}[width =\figwid]{story1.png}\put(6,15){T = 5 s}
%\end{overpic}
%\begin{overpic}[width =\figwid]{story2.png}\put(6,15){T = 12 s}
%\end{overpic}
%\begin{overpic}[width =\figwid]{story3.png}\put(6,15){T = 20 s}
%\end{overpic}
%\begin{overpic}[width =\figwid]{story4.png}\put(6,15){T = 25 s}
%\end{overpic}
%\begin{overpic}[width =\figwid]{story5.png}\put(6,15){T = 33 s}
%\end{overpic}}
\begin{overpic}[width =\columnwidth]{SwarmRun.pdf}\end{overpic}
\caption{\label{fig:story}Snapshots showing an object manipulation simulation with 100 robots under automatic control (see also Extension 1).
}
\end{figure}

\begin{figure}
\centering
\begin{overpic}[width=0.4\columnwidth]{MDPEmaze.pdf}\end{overpic}
\begin{overpic}[width=0.4\columnwidth]{EmazeFinal.pdf}\end{overpic}\\
\vspace{0.5em}
\begin{overpic}[width=0.4\columnwidth]{Spiral.pdf}\end{overpic}
\begin{overpic}[width=0.4\columnwidth]{SpiralSuccess.pdf}\end{overpic}
\caption{\label{fig:Emaze} We have tested multiple mazes to illustrate that the algorithm works for different environments.
}
\end{figure}
\begin{figure}
\centering
\begin{overpic}[width=\columnwidth]{DifferentShapes.pdf}\end{overpic}
\vspace{-2em}
\caption{\label{fig:DifferentMazes} Different maps and mazes were tested to validate the algorithm. E-shape maze has the least completion time average because it has the shortest path to the goal from the starting position.
}
\end{figure}

The interaction between the robots and object is impulsive so, like the study of impulsive pulling by Christensen  \cite{christensen2016let},  adding additional robots decreases completion time, but with diminishing returns. 
 After 75 robots, additional robots no longer can interact with the object and do not contribute to the task success. 
Brownian noise adds stochasticity.  This randomness can break the object free if it is stuck, but diminishes the effect of the control input.  
 Increasing noise increases completion time. 
 %Large amounts of noise caused failures, defined as trials lasting longer than 1000s.  With $W=50$, XX of 20 trials failed.  With $W=100$, XX of 20 trials failed.
The robots have limited force, so increasing the object weight increases completion time.  
Each shape was designed to have the same mass and area.
 Rectangles and squares tend to get stuck in the 90$^\circ$ workspace corners, and cause longer completion times than circles, triangles, and hexagons.






\begin{figure*}
\centering
\renewcommand{\figwid}{0.5\columnwidth}
\begin{overpic}[width =\figwid]{SimVeryNum.pdf}\put(1,55){a)}
\end{overpic}
\begin{overpic}[width =\figwid]{SimVeryNoise.pdf}\put(1,55){b)}
\end{overpic}
\begin{overpic}[width =\figwid]{SimVeryWeight.pdf}\put(1,55){c)}
\end{overpic}
\begin{overpic}[width =\figwid]{SimVeryShape.pdf}\put(1,55){d)}
\end{overpic}
\vspace{-0.5em}
\caption{\label{fig:AutoVeryParam}Parameter sweep simulation studies for a) number of robots, b) different noise values, c) object weight, and d) object shape.  Each bar is labelled with the number of trials. Completion time is in seconds.
%\vspace{-2em}
}
\end{figure*}






%Algorithm \ref{alg:BlockPushing} is an imperfect solution and has a failure mode if the robot swarm becomes multi-modal with modes separated by an obstacle, as shown in Fig.~\ref{fig:Failure}.  In this case, moving toward a corner will never reduce the variance below $\sigma_{min}^2$.


%  The first challenge is to identify when the distribution has become multi-modal.  Measuring just the mean and variance is insufficient to determine if a distribution is no longer unimodal, but if the swarm is being directed to a corner, and the variance does not reduce below $\sigma_{min}^2$, the swarm has become separated. In this case, we must either manipulate with a partial swarm, or run a gathering algorithm.  For the  {\sffamily S}-shaped workspace in this study, an open-loop input that commands the swarm to move in succession \{{\sc west, north, east, south}\} will move the swarm to the bottom right corner.
%This is not true for all obstacle fields. In a {\sffamily T}-shaped workspace, it is not possible to find an open-loop input that will move the entire swarm to the bottom of the {\sffamily T}.  
 
%  Using only the mean and variance may be overly restrictive.  Many heuristics using high-order moments have been developed to test if a distribution is multimodal~\cite{haldane1951simple}.  Often the sensor data itself, though it may not resolve individual robots, will indicate multi-modality.  For instance CCD images reveal clusters of bacteria, and MRI scans show agglomerations of particles~\cite{stuber2007positive}.  This data can be fitted with $k$-means or expectation maximization algorithms, and manipulation could be performed with the nearest swarm of sufficient size.
  








