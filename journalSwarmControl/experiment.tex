
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Particle Swarm Object manipulation}\label{sec:exp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section analyzes an \emph{object manipulation} task attempted by our hybrid, hysteresis-based controllers. The swarm must deliver the object to the goal region. 
%We assume that particles pushing the object move slower than unencumbered particles, and that pushing with more particles increases the object speed
We assume Coulomb and viscous friction parameters such that the object can be moved by particle motion.  Increasing the number of pushing particles increases the object speed.
%Although one particle might be able to move the object, it requires a long time for it to complete the task and probability of getting stuck when there is not enough particles is high.
 To solve this object manipulation task we divide the task into three components: 1) designing a policy for the object, 2) pushing the object with a compliant swarm, and 3) managing outliers.

The table below summarizes the simulation results for each 10 successful trials:
\begin{center}
\begin{tabular}{ c c }
\hline
Method & Result, mean$\pm$std (s)\\ [0.5ex] 
\hline \hline
Value Iteration (VI) & 367 $\pm$ 253 \\ 
\hline
VI + Potential Fields (PF) & 271 $\pm$ 267 \\  
\hline
VI + Outlier Rejection (OR) & 245 $\pm$ 135\\
\hline
BFS + PF + OR & 183 $\pm$ 179 \\
\hline
VI + PF + OR & 90 $\pm$ 35 \\[0.5ex]
\hline
\end{tabular}
\end{center}
\subsection{Learning a policy for the object}\label{subsec:objectpolicy}

To design the policy we first discretize the environment. 
In \cite{ShahrokhiIROS2015}, we used breadth-first search (BFS) on this discretized grid, but using workspace BFS fails to account for the hull of the object and will suggest moves that can cause collisions with the workspace. A configuration-space BFS approach avoids that problem but still fails to model uncertain actuation of the object by the swarm.

To solve both these problems, this paper models object movement as a Markov Decision Process (MDP) with non-deterministic movement.  
  Value Iteration is used to learn an \emph{optimal policy}~\cite{Thrun2005}.
   At each state the object can be commanded to move in one of eight directions with a small probability of moving in a wrong direction. 
 
% A corresponding reward function gives a high reward to the goal state,  
% a large negative reward to states including obstacles, and a small negative reward to all other states.
The reward function $r(x,\mathbf{u})$ is defined as
\begin{align}
r(x,\mathbf{u}) &=  \left\{
\begin{array}{ll}
     +100, &  \textrm{if } \mathbf{u} \textrm{ leads to goal state}\\
      -100, & \textrm{if } \mathbf{u} \textrm{ leads to an obstacle state} \\
      -1, & \textrm{otherwise}\\
\end{array} 
\right.
\end{align}
 where $x$ is the current state and $\mathbf{u}$ is the action.   %cumulative expected
  Value iteration computes $\hat{V}(x)$, the expected discounted sum reward if the optimal policy is implemented, for the object starting in each state $x$. The optimal policy is %the control action that, in probability, results in the largest value:
   \begin{align} \mathbf{D}(x) = \arg\max_{\mathbf{u}}  \left[ r(x,\mathbf{u}) + \sum\limits_{j=1}^N \hat{V}(x_j) p(x_j | x, \mathbf{u}) \right].  \label{eq:OptimalPolicy}
   \end{align}
   The value function $\hat{V}(x_i) $ is calculated by computing the value $\hat{V}$ for all $N$ states and iterating until convergence:
\begin{align}
\text{for }&\text{$i=1$ to $N$ do} \nonumber \\
&\hat{V} (x_i) = \gamma \max_{\mathbf{u}} \left[r(x_i,\mathbf{u}) + \sum\limits_{j=1}^N \hat{V}(x_j) p(x_j | x_i,\mathbf{u})\right] \label{eq:ValueIteration} \nonumber\\
\text{end}&  %page 502
\end{align}
Our probabilistic motion model $p(x_j | x, \mathbf{u})$ assumed the object moved in the commanded direction $\mathbf{u}$ half of the time but $+45^\circ$ with probability 0.25 and $-45^\circ$ with probability 0.25. 
In our experiments $\gamma = 0.97$, and \eqref{eq:ValueIteration} was iterated 200 times. A {\sc Matlab} implementation is available at \cite{Becker2015MDP}.
%Rather than a shortest distance to goal, $\nabla \mathbf{M}_{MDP}$  gives the approximate minimum cost to goal.

$\mathbf{M}_{\rm{BFS}}$ and the value function are shown in Fig.~\ref{fig:BFSGradient}. 
In 10 simulations with 100 particles, pushing the object to goal using BFS required 183$\pm$179 s while Value Iteration required 90$\pm$35 s (mean$\pm$std).
%\todo{(report results of 10 simulations with BFS and with PI.  Give mean and std)}

\begin{figure}
\centering
\renewcommand{\figwid}{3cm}
\begin{overpic}[height=\figwid]{GradientView.png}
\end{overpic}
\begin{overpic}[height=\figwid]{PolicyIter.png}
\end{overpic}
\begin{overpic}[height=\figwid]{MDPmap.pdf}
\end{overpic}
\vspace{-0.5em}
\caption{\label{fig:BFSGradient} BFS finds the shortest path for the moveable object to compute gradient vectors (left). Modeling as an MDP enables encoding penalties for being near obstacles. (Middle) The control policy from value iteration. (Right) The vision algorithm detects obstacles in the hardware setup. This map is used to produce the value function and control policy shown. 
%\vspace{-2em}
}
\end{figure}


\subsection{Potential fields for swarm management with a compliant manipulator}

When the swarm is in front of the object, control law \eqref{eq:PDcontrolPosition} pushes the object backwards.  To fix this, we implement a potential field approach that attracts the swarm to the intermediate goal, but repulses the swarm from in front of the object, as shown in Fig.~\ref{fig:potentialField}. The approach is similar to \cite{spong2008robot}, chapter 5.
The repulsive potential field is centered at $\mathbf{b}$, the object's COM, and is active in a circular sector of angular width 2$\theta$ and radius $d_0$ aligned with $\mathbf{D}(\mathbf{b})$. $\mathbf{D}(\mathbf{b})$ is the desired direction of motion from~\eqref{eq:OptimalPolicy}.
\begin{align}
\mathbf{d} &=  [\bar{x},\bar{y}] - \mathbf{b}, \qquad   \phi =\cos^{-1}\left( \frac{ \mathbf{D}(\mathbf{b}) \cdot  \mathbf{d}  }{\norm{\mathbf{D}(\mathbf{b})} \cdot \norm{\mathbf{d}} } \right), \nonumber \\ 
F_{\rm{att}} &= -\zeta \frac{\mathbf{d}  }{ \norm{\mathbf{d}}}, \nonumber\\
 F_{\rm{rep}} &=  \left\{
\begin{array}{ll}
      \eta( \frac{1}{\norm{\mathbf{d}}}- \frac{1}{d_0}) \frac{1}{\norm{\mathbf{d}}^2} \mathbf{d}, ~& \norm{\mathbf{d}} \leq d_0~\wedge~\phi <  \theta \\
      0, & \textrm{ otherwise} \nonumber \\
\end{array},
\right.\\
F_{\rm{pot}} &= F_{\rm{att}} + F_{\rm{rep}} \label{eq:potentialfield}, \nonumber\\
[x_{\rm{goal}}&, y_{\rm{goal}}] =  [\bar{x},\bar{y}]  + \frac{F_{\rm{pot}}}{\norm{F_{\rm{pot}}}}.
\end{align}
Here  $\eta$ and $\zeta$ are positive parameters that scale the forces and $\norm{\mathbf{d}}$ is the distance from the swarm mean $ [\bar{x},\bar{y}]$ to the object COM.
In simulations, $\theta =  \pi/2$,  $\eta  = 75$, $\zeta = 2$ and $d_0 = 3$. Because the kilobots have a slower time constant, they use $\theta =  \pi/2$,  $\eta  = 50$, $\zeta = 1$ and $d_0 = 7.5$. 

In 10 simulations with 100 particles, pushing the object to goal without a repulsive potential field failed in two of twelve runs. No failures occurred with the repulsive potential field.  Of successful trials, completion time without repulsive potential fields required 245$\pm$135 s while using repulsive potential fields required 90$\pm$35 s (mean$\pm$std).
%In 10 simulations with 100 robots, completion time without repulsive potential fields required $\mu=245, \sigma=135$s while using repulsive potential fields required $\mu=90, \sigma=35$.

%\todo{report results of 10 simulations with and without Potential fields.  Give mean and std}

\begin{figure}
\centering
\begin{overpic}[width=1\columnwidth]{PotentialField.pdf}\end{overpic}
%\todo{I like the 'target' symbol, but it is not self-documenting.  We need a legend explaining the min and max variance ellipses, the goal region, the variance, the mean, the object COM, and the target mean position.  I think these are easiest to make in powerpoint.
%Please use the same color and line style for the variance min and max as you use in Figure 4.
%}
%{blockpushingImageWithMeanAndVarianceOverlay.png}
\caption{\label{fig:potentialField} (Left) The attractive field is centered behind the object's COM. (Middle) The repulsive field is centered at the object's COM. (Right) Combining these forces prevents the swarm from pushing the object backwards.}
\end{figure}

\subsection{Outlier rejection}\label{subsec:OutlierRejection}

The variance controller in Alg.~\ref{alg:MeanVarianceControl} is a greedy algorithm that is susceptible to outliers. 
Our controller in \cite{ShahrokhiIROS2015} failed in $14\%$ trials, in each failure some particles were unable to reach the object because workspace obstacles were blocking them. This failure rate increases if  object weight increases or ground-particle friction increases. The mean and covariance calculations \eqref{eq:meanVar} included all particles in the workspace. Particles that cannot reach the object due to obstacles skew these calculations. The state machine in Fig. \ref{fig:Region}a solves this problem by creating two states for the maze: either main or transfer. Each state has a set of regions representing a discretized visibility polygon. Whenever the object crosses a region boundary the state toggles. The \emph{main} regions are generated by extending obstacles until they meet another obstacle. The \emph{transfer} regions are perpendicular to obstacle boundaries, and act as a buffer between two main regions.

Fig.\ \ref{fig:Region}b shows the regions for the main state. The object is in region 1. An indicator function is applied to \eqref{eq:meanVar} so only particles inside region 1 are counted.  This filtering increases experimental success because the mean calculation only includes nearby particles that can directly interact with the object. 
When the object leaves main region 1 the state switches to transfer. The transfer regions are shown in Fig.\ \ref{fig:Region}c.  The object is in transfer region 1, so only particles in transfer region 1 are included in the mean and covariance calculations. 
 The particles should push the object to the left. Without filtering using regions, the red circle is the mean and the algorithm would instruct the particles to push the object up. The black circle shows the filtered mean and the algorithm instructs the particles to push the object directly left.

In 10 simulations with 100 particles, completion time without outlier rejection required 271$\pm$267 s while using outlier rejection required 90$\pm$35 s (mean$\pm$std).
%\todo{report results of 10 simulations with and without regions.  Give mean and std}


\begin{figure}
\begin{center}
	\begin{overpic}[width=0.9\columnwidth]{StateMachine.pdf}\put(-4,25){\textbf{a}}\end{overpic}\\
	~~\begin{overpic}[width=0.45\columnwidth]{MainRegions.pdf} %,grid=true
	\put(-6,85){\textbf{b}}
	\end{overpic}
	~~
	\begin{overpic}[width=0.45\columnwidth]{TransferRegions.pdf}
	\put(-6,85){\textbf{c}}
	\end{overpic}
\end{center}
\vspace{-0.5em}
\caption{\label{fig:Region}  Outlier rejection state machine and regions.
}
\end{figure}

\subsection{Simulation results}\label{sec:AlgObjectManipulation}
 We use the hybrid hysteresis-based controller in Alg.~\ref{alg:MeanVarianceControl}  to track the desired position, while maintaining sufficient particle density to move the object by switching to minimize variance whenever variance exceeds a set limit:  $0.003 W$ and $0.006 W$ were added to the min and max variance limits from \eqref{eq:sigMaxMin}, where $W$ is the magnitude of the Brownian noise.
 The minimize variance control law \eqref{eq:PDcontrolVariance} is slightly modified to choose the nearest corner further from the goal than the object with an obstacle-free straight-line path to the object. 
The control algorithm  for object manipulation is listed in Alg.~\ref{alg:BlockPushing}. 

In rare cases during simulations the swarm may become trapped in a local minimum of \eqref{eq:potentialfield}.
If the swarm mean position does not change for five seconds, the swarm is assumed to be in a local minimum and is commanded to move toward the previous corner. As soon as the mean position changes, normal control resumes.

\begin{figure}
%  \vspace{-20pt}
  \begin{center}
\begin{overpic}[width=0.4\columnwidth]{AllShapes.pdf}\end{overpic}
  \end{center}
%  \vspace{-1em}
\caption{\label{fig:Shapes} The six equal-area objects tested in simulation. %Six object shapes were tested.
\vspace{-1em}
}
\end{figure}



%\todo{add the timeout function: I added two lines.}

\begin{algorithm}
\caption{Object-manipulation controller for a particle swarm.}\label{alg:BlockPushing}
\begin{algorithmic}[1]
\Require Knowledge of moveable object's center of mass $\mathbf{b}$; swarm mean $[\bar{x},\bar{y}]$ and variance $[\sigma_x^2, \sigma_y^2]$, each calculated using the regions function from \S \ref{subsec:OutlierRejection};  map of the environment
\State Compute optimal policy for object, according to \S \ref{subsec:objectpolicy}
\While{ $\mathbf{b}$ is not in goal region}
\State $\sigma^2 \gets \max{(\sigma_x,\sigma_y)}$
\If {$\sigma^2 > \sigma_{\rm{max}}^2$}
\While{$\sigma^2 > \sigma_{\rm{min}}^2$}
\State $ [x_{\rm{goal}}, y_{\rm{goal}}] \gets $ nearest corner in region
\State Apply \eqref{eq:PDcontrolPosition} to move toward $[x_{\rm{goal}}, y_{\rm{goal}}]$
\EndWhile
\Else  
%\If {$\mathrm{distance}( \mathbf{b}, [x_{goal}, y_{goal}] ) > k_1 r_b$}
%	\State$r_p \gets k_2 r_b$  \Comment{guarded move}
%	\Else
%	\State$r_p \gets k_3 r_b$  \Comment{pushing move}
%	\EndIf
%\State$r_p \gets k_3 r_b$  \Comment{pushing move}
\State Calculate $\mathbf{D}(\mathbf{b})$  \Comment{direction for object at $\mathbf{b}$}
\State Apply \eqref{eq:potentialfield}   \Comment{potential field for swarm}
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}


%\subsection{Automated object manipulation (simulation)}
Fig.~\ref{fig:story} shows snapshots during an execution of this algorithm in simulation.
 To illustrate the flexibility of the algorithm we tested two additional workspaces, \emph{{\sffamily E}-shaped} and  \emph{Spiral}, without changing the algorithm. These are shown in Fig.~\ref{fig:Emaze}. 
 More complicated workspaces could be generated by composing these workspaces.
 Fig.~\ref{fig:DifferentMazes} shows the results of all  three mazes. The {\sffamily E}-shaped maze required the least average time because the path to the goal is shorter. 
   Experimental results of parameters sweeps are summarized in Fig.~\ref{fig:AutoVeryParam}.  Each trial measured the time to deliver the object to the goal location.  The default parameter settings used 100 particles, a normalized weight of 1, a hexagon shape, and Brownian noise (applied once each simulation step) with $W=5$.  

\begin{figure}
\centering

\begin{overpic}[width =\columnwidth]{SwarmRun.pdf}\end{overpic}
\caption{\label{fig:story}Snapshots showing an object manipulation simulation with 100 particles under automatic control (see also Extension 1).
}
\end{figure}

\begin{figure}\begin{center}\begin{overpic}[width=0.4\columnwidth]{MDPEmaze.pdf}\end{overpic}~\begin{overpic}[width=0.4\columnwidth]{EmazeFinal.pdf}\end{overpic}\\
\vspace{.25em}
\begin{overpic}[width=0.4\columnwidth]{Spiral.pdf}\end{overpic}~\begin{overpic}[width=0.4\columnwidth]{SpiralSuccess.pdf}\end{overpic}
\end{center}
\caption{\label{fig:Emaze} We tested three workspaces. The control policies from Value Iteration for an {\sffamily E}-shaped and a spiral workspace are shown in the left column.
}
\end{figure}
\begin{figure}
\centering
\begin{overpic}[width=.5\columnwidth]{DifferentShapes.pdf}\end{overpic}
\vspace{-1em}
\caption{\label{fig:DifferentMazes} Completion times for three workspaces.}
\end{figure}

The interaction between the particles and object is impulsive so, like the study of impulsive pulling in \cite{christensen2016let},  adding additional particles decreases completion time, but with diminishing returns. 
 The effect of adding particles diminishes asymptotically because additional particles have difficulty interacting with the object. 
Brownian noise adds stochasticity.  This randomness can break the object free if it is stuck, but diminishes the effect of the control input.  
 Increasing noise increases completion time. 
 %Large amounts of noise caused failures, defined as trials lasting longer than 1000s.  With $W=50$, XX of 20 trials failed.  With $W=100$, XX of 20 trials failed.
The particles have limited force, so increasing the object weight increases completion time.  
Each shape was designed to have the same mass and area.
 Rectangles and squares tend to get stuck in the 90$^\circ$ workspace corners, and cause longer completion times than circles, triangles, and hexagons.






\begin{figure*}
\centering
\renewcommand{\figwid}{0.5\columnwidth}
\begin{overpic}[width =\figwid]{SimVeryNum.pdf}\put(1,55){a)}
\end{overpic}
\begin{overpic}[width =\figwid]{SimVeryNoise.pdf}\put(1,55){b)}
\end{overpic}
\begin{overpic}[width =\figwid]{SimVeryWeight.pdf}\put(1,55){c)}
\end{overpic}
\begin{overpic}[width =\figwid]{SimVeryShape.pdf}\put(1,55){d)}
\end{overpic}
\vspace{-0.5em}
\caption{\label{fig:AutoVeryParam}Parameter sweep simulation studies for a) number of particles, b) different noise values, c) object weight, and d) object shape.  Each bar is labelled with the number of trials. Completion time is in seconds.
%\vspace{-2em}
}
\end{figure*}













