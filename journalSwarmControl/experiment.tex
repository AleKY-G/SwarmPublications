
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Object manipulation results}\label{sec:exp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section analyzes an \emph{object manipulation} task attempted by both our hybrid, hysteresis-based controller, inspired by the analysis of human users in Section~\ref{sec:expResults}.  
The swarm must deliver the object to the goal region.  To solve this object manipulation task, we divide the task into two components: designing a policy for the object, and designing a control law for the swarm to push the object according to the policy.

\subsection{Learning a policy for the object}\label{subsec:objectpolicy}

To design the policy, we first discretize the environment. 
In \cite{ShahrokhiIROS2015}, we used breadth-first search (BFS) on this discretized grid to determine $\mathbf{M}_{BFS}$, the shortest distance from any grid cell to the goal, and generated a gradient map $\nabla \mathbf{M}$  shown in Fig.~\ref{fig:BFSGradient}.  
The object's center of mass is at $\mathbf{b}$ and has average radius $r_b$, so we define the desired direction for the object as $\mathbf{D}(\mathbf{b}) = \nabla \mathbf{M}(\mathbf{b})$. 
The robots were directed behind the object at  $\mathbf{b} - 1.5 r_b \mathbf{D}(\mathbf{b})$, then directed to  $\mathbf{b} - 0.1 r_b \mathbf{D}(\mathbf{b})$ to push the object toward the goal location. However, BFS has no penalties on object collisions with obstacles, and the resulting collisions made the BFS solution slow.

 To avoid obstacles, this paper models object movement as a Markov Decision Process (MDP) with non-deterministic movement.  
  Value iteration,  as described in \cite{Thrun2005}, is used to learn an \emph{optimal policy}: a function that assigns a control input to every possible state.
 At each state the object can be commanded to move in one of eight directions with a small probability of moving in the wrong direction. 
 
 A corresponding reward function gives a high reward to the goal state,  
 a large negative reward to states including obstacles, and a small negative reward to all other states.
The reward function $r(x,u)$ is defined as
\begin{align}
r(x,u) &=  \left\{
\begin{array}{ll}
     +100, &  \textrm{if } u \textrm{ leads to goal state}\\
      -100, & \textrm{if } u \textrm{ leads to a state with an obstacle} \\
      -1, & \textrm{otherwise}\\
\end{array} 
\right.
\end{align}
where $u$ is the action and $x$ is the current state. 
  Value iteration iteratively learns the value $\hat{V}(x)$ for the object being in each state $x$. The optimal policy is the control action that, in probability, results in the largest value:
   \begin{align} \mathbf{D}(x) = \arg\max_u   [ r(x,u) + \sum\limits_{j=1}^N \hat{V}(x_j) p(x_j| u,x)].  \label{eq:OptimalPolicy}
   \end{align}
   The value function $\hat{V}(x_j) $ is calculated by computing the value $\hat{V}$ for all $N$ states and iterating until convergence:
\begin{align}
\text{for }&\text{$i=1$ to $N$ do} \nonumber \\
&\hat{V} (x_j) = \gamma \max_u [r(x_j,u) + \sum\limits_{i=1}^N \hat{V}(x_i) p(x_i| u,x_j)] \label{eq:ValueIteration}\\
\text{end}& \nonumber
\end{align}
In our experiments $\gamma = 0.97$, and \eqref{eq:ValueIteration} was iterated 200 times. A {\sc Matlab} implementation of this algorithm is available at \href{https://www.mathworks.com/matlabcentral/fileexchange/49992-mdp-robot-grid-world-example}{\emph{MDP grid world example}}.%\todo{link to algorithm}.  
%Rather than a shortest distance to goal, $\nabla \mathbf{M}_{MDP}$  gives the approximate minimum cost to goal.

$\mathbf{M}_{BFS}$ and the value function are shown in Fig.~\ref{fig:BFSGradient}. 
In 10 simulations with 100 robots, pushing the object to goal using BFS required a mean and standard deviation of $183\pm 179$ s while Policy Iteration required $90\pm 35$ s.
%\todo{(report results of 10 simulations with BFS and with PI.  Give mean and std)}

\begin{figure}
\centering
\begin{overpic}[scale=0.19]{GradientView.png}
\end{overpic}
\begin{overpic}[scale=0.19]{PolicyIter.png}
\end{overpic}
\begin{overpic}[scale=0.262]{MDPmap.pdf}
\end{overpic}
\vspace{-0.5em}
\caption{\label{fig:BFSGradient}The BFS algorithm finds the shortest path for the moveable object  to compute gradient vectors (left). Modeling as an MDP enables encoding penalties for being near obstacles. (Middle) The control policy from value iteration. (Right) The vision algorithm detects obstacles, used to produce the
value function and control policy for the hardware setup. 
%\vspace{-2em}
}
\end{figure}


\subsection{Potential fields for swarm management with a compliant manipulator}

Unfortunately, when the swarm is in front of the object, control law \eqref{eq:PDcontrolPosition} pushes the object backwards.  To fix this, we implement a potential field approach \cite{spong2008robot} that attracts the swarm to the intermediate goal, but repulses the swarm from in front of the object.
The repulsive potential field is centered at the object's COM and is active for a radius $\rho_0$, but is implemented only when the swarm mean is within $\theta$ of the desired direction of motion $\mathbf{D}$ as shown in Fig.~\ref{fig:potentialField}.
\begin{align}
F_{att} &= -\zeta \Delta \rho / \rho \\
%\mathbf{u} &= [\bar{x},\bar{y}] - \mathbf{b} \nonumber \\
\phi &=\cos^{-1}\left( \frac{ \mathbf{D} \cdot ( [\bar{x},\bar{y}] - \mathbf{b}) }{\norm{\mathbf{D}} \norm{ [\bar{x},\bar{y}] - \mathbf{b}} } \right) \\
%\theta &= \mathrm{angularDistance}\left( \alpha, \beta \right)\nonumber \\
 F_{rep} &=  \left\{
\begin{array}{ll}
      \eta( 1/\rho- 1/\rho_0) \frac{1}{\rho^2} \Delta \rho, ~& \rho\leq \rho_0~\&~\phi <  \theta \\
      0, & \textrm{ otherwise} \\
\end{array} 
\right.\\
F_{pot} &= F_{att} + F_{rep} \label{eq:potentialfield}
\end{align}

In simulations, $\theta =  \pi/2$,  $\eta  = 75$, $\zeta = 2$ and $\rho_0 = 3$. Because the kilobot hardware experiments have a slower time constant, they use $\theta =  \pi/2$,  $\eta  = 50$, $\zeta = 1$ and $\rho_0 = 7.5$. 

In 10 simulations with 100 robots, pushing the object to goal without a repulsive potential field failed in two of twelve runs. No failures occurred with the repulsive potential field.  Of successful trials, completion time without repulsive potential fields required $\mu=245, \sigma=135$ s while using repulsive potential fields required $\mu=90, \sigma=35$ s.
%In 10 simulations with 100 robots, completion time without repulsive potential fields required $\mu=245, \sigma=135$s while using repulsive potential fields required $\mu=90, \sigma=35$.

%\todo{report results of 10 simulations with and without Potential fields.  Give mean and std}

\begin{figure}
\centering
\begin{overpic}[width=1\columnwidth]{PotentialField.pdf}\end{overpic}
%\todo{I like the 'target' symbol, but it is not self-documenting.  We need a legend explaining the min and max variance ellipses, the goal region, the variance, the mean, the object COM, and the target mean position.  I think these are easiest to make in powerpoint.
%Please use the same color and line style for the variance min and max as you use in Figure 4.
%}
%{blockpushingImageWithMeanAndVarianceOverlay.png}
\caption{\label{fig:potentialField} (Left) The attractive field is centered behind the object's COM. (Middle) The repulsive field is centered at the object's COM. (Right) Combining these forces prevents the swarm from pushing the object backwards.}
\end{figure}

\subsection{Outlier rejection}\label{subsec:OutlierRejection}

The variance controller in Alg.~\ref{alg:MeanVarianceControl} is a greedy algorithm that is susceptible to outliers. The controller in \cite{Shahrokhi2015} failed in $14\%$ trials, often because workspace obstacles made some robots unable to reach the object. This failure rate increases if  object weight increases or ground-robot friction increases. The mean and covariance calculations \eqref{eq:meanVar} included all robots in the workspace. Robots that cannot reach the object due to obstacles skew these calculations. The state machine in Fig.\ \ref{fig:Region}.a solves this problem by creating two states for the maze: either main or transfer. Each state has a set of regions representing a discretized visibility polygon. Whenever the object crosses a region boundary the state toggles. The main regions are generated by extending obstacles until they meet another obstacle shown in Fig.~\ref{fig:Region}.b. The transfer regions are perpendicular to obstacle boundaries, and act as a buffer between two main regions shown in Fig.~\ref{fig:Region}.c.
This filtering increases experimental success because the mean calculation only includes nearby robots that can directly interact with the object. In the example, we want the robots to push the object to the right. Without filtering the robots, the orange star is the mean and the algorithm would instruct the robots to push the object southeast. The filtered mean is at the yellow star and the algorithm instructs the robots to push the object directly east. 
%When the object leaves main region 1 the maze state switches to transfer. 
%The object is in transfer region 0, so only robots in transfer region 0 are included in the mean and covariance calculations.  
%This heuristic improves performance by $50\%$ or less regarding the object heaviness.
In 10 simulations with 100 robots, completion time without outlier rejection required $\mu=245, \sigma=135$ s while using outlier rejection required $\mu=90, \sigma=35$ s.
%\todo{report results of 10 simulations with and without regions.  Give mean and std}


\begin{figure*}
\begin{center}
	\begin{overpic}[width=0.45\columnwidth]{mainRegions.pdf}\end{overpic}
	\begin{overpic}[width=0.45\columnwidth]{transferRegions.pdf}\end{overpic}\\
	\vspace{0.5em}
	\begin{overpic}[width=0.6\columnwidth]{stateMachine.pdf}\end{overpic}
\end{center}
\caption{\label{fig:Region}  Outlier rejection state machine and regions.
}
\end{figure*}

\subsection{Algorithm for object manipulation}
 We use the hybrid hysteresis-based controller in Alg.~\ref{alg:MeanVarianceControl}  to track the desired position, while maintaining sufficient robot density to move the object by switching to minimize variance whenever variance exceeds a set limit. The minimize variance control law \eqref{eq:PDcontrolVariance} is slightly modified to choose the nearest corner further from the goal than $\mathbf{b}$ with an obstacle-free straight-line path to $\mathbf{b}$. 
The control algorithm  for object manipulation is listed in Alg.~\ref{alg:BlockPushing}. 

In rare cases during simulations the swarm may become trapped in a local minima of \eqref{eq:potentialfield}.
If the swarm mean position does not change for ten seconds, the swarm is assumed to be in a local minima and is commanded to move toward the previous corner. As soon as the mean position changes, normal control resumes.

\begin{wrapfigure}{R}{0.3\textwidth}
  \vspace{-20pt}
  \begin{center}
\begin{overpic}[width=0.3\columnwidth]{AllShapes.pdf}\end{overpic}
  \end{center}
  \vspace{-1em}
\caption{\label{fig:Shapes} Six equal-area objects were tested. %Six object shapes were tested.
\vspace{-1em}
}
\end{wrapfigure}



%\todo{add the timeout function: I added two lines.}

\begin{algorithm}
\caption{Object-manipulation controller for a robotic swarm.}\label{alg:BlockPushing}
\begin{algorithmic}[1]
\Require Knowledge of moveable object's center of mass $\mathbf{b}$; swarm mean $[\bar{x},\bar{y}]$ and variance $[\sigma_x^2, \sigma_y^2]$, each calculated using the regions function from \S \ref{subsec:OutlierRejection};  map of the environment
\State Compute optimal policy for object, according to \S \ref{subsec:objectpolicy}
\While{$\mathbf{b}$ is not in goal region}
\State $\sigma^2 \gets \max{(\sigma_x,\sigma_y)}$
\If {$\sigma^2 > \sigma_{max}^2$}
\While{$\sigma^2 > \sigma_{min}^2$}
\State $ [x_{goal}, y_{goal}] \gets $ the nearest corner in current region
\State Apply \eqref{eq:PDcontrolPosition} to move toward $[x_{goal}, y_{goal}]$
\EndWhile
\Else  
%\If {$\mathrm{distance}( \mathbf{b}, [x_{goal}, y_{goal}] ) > k_1 r_b$}
%	\State$r_p \gets k_2 r_b$  \Comment{guarded move}
%	\Else
%	\State$r_p \gets k_3 r_b$  \Comment{pushing move}
%	\EndIf
%\State$r_p \gets k_3 r_b$  \Comment{pushing move}
\State Calculate $\mathbf{D}(\mathbf{b})$  \Comment{direction for object at $\mathbf{b}$}
\State Apply \eqref{eq:potentialfield}   \Comment{potential field to push object in correct direction}
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}


\subsection{Automated object manipulation (simulation)}
Fig.~\ref{fig:story} shows snapshots during an execution of this algorithm in simulation. 
Experimental results of parameters sweeps are summarized in Fig.~\ref{fig:AutoVeryParam}.  Each trial measured the time to deliver the object to the goal location.  The default parameter settings used 100 robots, a normalized weight of 1, a hexagon shape, and Brownian noise (applied each simulation step) with $W=5$.  

\begin{figure*}
\centering
%\renewcommand{\figwid}{0.19\columnwidth}
%\href{http://youtu.be/tCej-9e6-4o}{\begin{overpic}[width =\figwid]{story1.png}\put(6,15){T = 5 s}
%\end{overpic}
%\begin{overpic}[width =\figwid]{story2.png}\put(6,15){T = 12 s}
%\end{overpic}
%\begin{overpic}[width =\figwid]{story3.png}\put(6,15){T = 20 s}
%\end{overpic}
%\begin{overpic}[width =\figwid]{story4.png}\put(6,15){T = 25 s}
%\end{overpic}
%\begin{overpic}[width =\figwid]{story5.png}\put(6,15){T = 33 s}
%\end{overpic}}
\begin{overpic}[width =\columnwidth]{SwarmRun.pdf}
\end{overpic}
\vspace{-2em}
\caption{\label{fig:story}\href{http://youtu.be/tCej-9e6-4o}{Snapshots showing an object manipulation simulation with 100 robots under automatic control.  See animation in~\cite{ShivaVideo2015}.}
%\vspace{-2em}
}
\end{figure*}

The interaction between the robots and object is impulsive so, like the study of impulsive pulling in  \cite{christensen2016let},  adding additional robots decreases completion time, but with diminishing returns. 
 After 75 robots, additional robots no longer can interact with the object and do not contribute to the task success. 
Brownian noise adds stochasticity.  This randomness can break the object free if it is stuck, but diminishes the effect of the control input.  
 Increasing noise increases completion time. 
 %Large amounts of noise caused failures, defined as trials lasting longer than 1000s.  With $W=50$, XX of 20 trials failed.  With $W=100$, XX of 20 trials failed.
The robots have limited force, so increasing the object weight increases completion time.  
Each shape was designed to have the same mass and area.
 Rectangles and squares tend to get stuck in the 90$^\circ$ workspace corners, and cause longer completion times than circles, triangles, and hexagons.






\begin{figure*}
\centering
\renewcommand{\figwid}{0.5\columnwidth}
\begin{overpic}[width =0.45\columnwidth]{SimVeryNum.pdf}\put(1,55){a)}
\end{overpic}
\begin{overpic}[width =0.45\columnwidth]{SimVeryNoise.pdf}\put(1,55){b)}
\end{overpic}
\begin{overpic}[width =0.45\columnwidth]{SimVeryWeight.pdf}\put(1,55){c)}
\end{overpic}
\begin{overpic}[width =0.45\columnwidth]{SimVeryShape.pdf}\put(1,55){d)}

\end{overpic}
\vspace{-0.5em}
\caption{\label{fig:AutoVeryParam}Parameter sweep for a) number of robots, b) different noise values, c) object weight, and d) object shape.  Each bar is labelled with the number of trials. Completion time is in seconds.
%\vspace{-2em}
}
\end{figure*}






%Algorithm \ref{alg:BlockPushing} is an imperfect solution and has a failure mode if the robot swarm becomes multi-modal with modes separated by an obstacle, as shown in Fig.~\ref{fig:Failure}.  In this case, moving toward a corner will never reduce the variance below $\sigma_{min}^2$.


%  The first challenge is to identify when the distribution has become multi-modal.  Measuring just the mean and variance is insufficient to determine if a distribution is no longer unimodal, but if the swarm is being directed to a corner, and the variance does not reduce below $\sigma_{min}^2$, the swarm has become separated. In this case, we must either manipulate with a partial swarm, or run a gathering algorithm.  For the  {\sffamily S}-shaped workspace in this study, an open-loop input that commands the swarm to move in succession \{{\sc west, north, east, south}\} will move the swarm to the bottom right corner.
%This is not true for all obstacle fields. In a {\sffamily T}-shaped workspace, it is not possible to find an open-loop input that will move the entire swarm to the bottom of the {\sffamily T}.  
 
%  Using only the mean and variance may be overly restrictive.  Many heuristics using high-order moments have been developed to test if a distribution is multimodal~\cite{haldane1951simple}.  Often the sensor data itself, though it may not resolve individual robots, will indicate multi-modality.  For instance CCD images reveal clusters of bacteria, and MRI scans show agglomerations of particles~\cite{stuber2007positive}.  This data can be fitted with $k$-means or expectation maximization algorithms, and manipulation could be performed with the nearest swarm of sufficient size.
  








